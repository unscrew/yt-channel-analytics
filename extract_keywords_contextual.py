 #!/usr/bin/env python3
"""
ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ STT ìµœì í™”)
LLMì²˜ëŸ¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import argparse
import json
from collections import Counter
import re
import os
import sys
import subprocess

# JAVA_HOME ìë™ ì„¤ì • (KoNLPyìš©)
def setup_java_home():
    """JAVA_HOME ìë™ ê°ì§€ ë° ì„¤ì •"""
    if os.environ.get('JAVA_HOME'):
        return True
    
    java_home = None
    
    if sys.platform == 'darwin':  # Mac
        # ë°©ë²• 1: /usr/libexec/java_home
        try:
            result = subprocess.run(['/usr/libexec/java_home'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                java_home = result.stdout.strip()
        except:
            pass
        
        # ë°©ë²• 2: Homebrew ê²½ë¡œë“¤
        if not java_home:
            homebrew_paths = [
                '/opt/homebrew/opt/openjdk/libexec/openjdk.jdk/Contents/Home',
                '/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home',
                '/usr/local/opt/openjdk/libexec/openjdk.jdk/Contents/Home',
                '/usr/local/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home',
                '/Library/Java/JavaVirtualMachines/jdk-21.jdk/Contents/Home',
                '/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home',
                '/Library/Java/JavaVirtualMachines/jdk-11.jdk/Contents/Home',
            ]
            for path in homebrew_paths:
                if os.path.exists(path):
                    java_home = path
                    break
    
    elif sys.platform.startswith('linux'):  # Linux
        linux_paths = [
            '/usr/lib/jvm/java-21-openjdk-amd64',
            '/usr/lib/jvm/java-17-openjdk-amd64',
            '/usr/lib/jvm/java-11-openjdk-amd64',
            '/usr/lib/jvm/default-java',
        ]
        for path in linux_paths:
            if os.path.exists(path):
                java_home = path
                break
    
    if java_home:
        os.environ['JAVA_HOME'] = java_home
        return True
    
    return False

# Java ì„¤ì • ì‹œë„
JAVA_AVAILABLE = setup_java_home()


def extract_metadata_and_content(text: str) -> tuple:
    """
    ë©”íƒ€ë°ì´í„°ì™€ ë³¸ë¬¸ ì™„ì „ ë¶„ë¦¬
    """
    lines = text.split('\n')
    separator_found = False
    content_start = 0
    
    for i, line in enumerate(lines):
        if '-' * 10 in line:  # êµ¬ë¶„ì„ 
            separator_found = True
            content_start = i + 1
            break
        if i > 10 and not separator_found:  # ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´
            content_start = 0
            break
    
    content = '\n'.join(lines[content_start:])
    return content


def extract_nouns_with_konlpy(text: str) -> list:
    """
    í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ëª…ì‚¬ë§Œ ì¶”ì¶œ (ê°€ì¥ ì •í™•)
    """
    if not JAVA_AVAILABLE:
        print("     âš ï¸ Java ì—†ìŒ - ê°„ë‹¨í•œ íŒ¨í„´ ì‚¬ìš©")
        print("     ğŸ’¡ Java ì„¤ì¹˜: brew install openjdk@11")
        return None
    
    try:
        from konlpy.tag import Okt
        okt = Okt()
        nouns = okt.nouns(text)
        return [n for n in nouns if len(n) >= 2]
    except ImportError:
        print("     âš ï¸ konlpy ì—†ìŒ - ê°„ë‹¨í•œ íŒ¨í„´ìœ¼ë¡œ ëŒ€ì²´")
        print("     ğŸ’¡ ì„¤ì¹˜: pip install konlpy")
        return None
    except Exception as e:
        print(f"     âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        print("     ğŸ’¡ Java ì„¤ì¹˜: brew install openjdk@11")
        return None


def extract_nouns_simple(text: str) -> list:
    """
    í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ ëª…ì‚¬ ì¶”ì¶œ (ë°±ì—…)
    ë‹¨ìˆœí•˜ê³  ê´€ëŒ€í•œ ë°©ì‹
    """
    # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
    words = re.findall(r'[ê°€-í£]{2,}', text)
    
    # ì „í˜€ í•„í„°ë§ ì•ˆ í•¨! ì¼ë‹¨ ëª¨ë‘ ë°˜í™˜
    # (ë¶ˆìš©ì–´ ì œê±°ëŠ” ë‚˜ì¤‘ì— ë³„ë„ë¡œ)
    return words


def get_extended_stopwords() -> set:
    """
    í™•ì¥ëœ ë¶ˆìš©ì–´ ì‚¬ì „ (í•œêµ­ì–´ êµ¬ì–´ì²´ íŠ¹í™”)
    """
    return {
        # ì§€ì‹œì–´
        'ì´ê±°', 'ê·¸ê±°', 'ì €ê±°', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ê²Œ', 'ê·¸ê²Œ', 'ì €ê²Œ',
        'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ìª½', 'ê·¸ìª½', 'ì €ìª½', 'ìš”ê¸°', 'ê±°ê¸°ì„œ',
        
        # ëŒ€ëª…ì‚¬
        'ë‚˜', 'ë„ˆ', 'ì €', 'ìš°ë¦¬', 'ì €í¬', 'ê·¸', 'ì´', 'ì €', 'ëˆ„êµ¬', 'ë­',
        'ë‚´ê°€', 'ë„¤ê°€', 'ì €ê°€', 'ì œê°€', 'ìš°ë¦¬ê°€', 'ì €í¬ê°€',
        
        # ì ‘ì†ì‚¬/ë¶€ì‚¬
        'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¼ë°', 'ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´',
        'ê·¸ëƒ¥', 'ì¢€', 'ë§‰', 'ì§„ì§œ', 'ì •ë§', 'ë„ˆë¬´', 'ë˜ê²Œ', 'ì—„ì²­', 'ì™„ì „',
        'ì•½ê°„', 'ì¡°ê¸ˆ', 'ë§ì´', 'ì•„ì£¼', 'êµ‰ì¥íˆ', 'ì •ë§ë¡œ',
        
        # ì‹œê°„/ìˆœì„œ
        'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'ì´ì œ', 'ë‚˜ì¤‘', 'ë¨¼ì €', 'ë‹¤ìŒ',
        'ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ì–¸ì œ', 'í•­ìƒ', 'ê°€ë”', 'ìì£¼',
        
        # ë™ì‚¬ íŒŒìƒ
        'ë˜ëŠ”', 'í•˜ëŠ”', 'í•œë‹¤', 'í–ˆë‹¤', 'í• ', 'ëœ', 'í–ˆê³ ', 'í•˜ê³ ',
        'ìˆëŠ”', 'ì—†ëŠ”', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
        
        # í˜•ìš©ì‚¬ íŒŒìƒ
        'ì¢‹ì€', 'ë‚˜ìœ', 'í¬ë‹¤', 'ì‘ë‹¤', 'ë§ë‹¤', 'ì ë‹¤',
        
        # ì¡°ì‚¬ë¥˜
        'ë•Œë¬¸', 'ê²½ìš°', 'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë“±', 'ë°', 'ë•Œ', 'ë„', 'ë§Œ',
        
        # ê°íƒ„ì‚¬/ì¶”ì„ìƒˆ
        'ì•„ë‹ˆ', 'ì•„ë‹ˆì•¼', 'ì•„ë‹ˆìš”', 'ë„¤', 'ì˜ˆ', 'ì‘', 'ì–´', 'ìŒ', 'ì˜¤',
        'ì™€', 'ìš°ì™€', 'í—', 'ëŒ€ë°•',
        
        # ì˜ë¬¸/í™•ì¸
        'ë­', 'ë­”ê°€', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–´ë””', 'ì–¸ì œ',
        
        # ë¬¸ì¥ ì—°ê²°
        'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°',
        
        # ê¸°íƒ€ êµ¬ì–´ì²´
        'ê±°ì˜', 'ì‚¬ì‹¤', 'ë³´í†µ', 'ì›ë˜', 'ì¼ë‹¨', 'ë¨¼ì €', 'ë‹¤ìŒ',
        'ë§', 'ì–˜ê¸°', 'ì´ì•¼ê¸°', 'ìƒê°', 'ëŠë‚Œ', 'ê°™ì€', 'ê°™ë‹¤',
        'í•˜ë©´', 'í•˜ë˜', 'í–ˆë˜', 'í• ê²Œ', 'í• ê¹Œ', 'í•˜ì',
        
        # ì˜ì–´ ë©”íƒ€ë°ì´í„°
        'video', 'title', 'model', 'whisper', 'base', 'id',
        'hqcnw', 'asmr'  # ì´ëŸ° ê±´ ì¼€ì´ìŠ¤ë³„ë¡œ íŒë‹¨
    }


def filter_by_frequency_and_length(words: list, min_freq: int = 2, min_len: int = 2) -> Counter:
    """
    ë¹ˆë„ì™€ ê¸¸ì´ë¡œ í•„í„°ë§
    """
    word_counts = Counter(words)
    
    # í•„í„°ë§
    filtered = {
        word: count 
        for word, count in word_counts.items()
        if count >= min_freq and len(word) >= min_len
    }
    
    return Counter(filtered)


def detect_named_entities_simple(text: str) -> dict:
    """
    ê°„ë‹¨í•œ ê³ ìœ ëª…ì‚¬ íƒì§€
    """
    entities = {
        'brands': [],
        'persons': [],
        'products': [],
        'topics': []
    }
    
    text_lower = text.lower()
    
    # ë¸Œëœë“œ/ì œí’ˆëª… (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ê±°ë‚˜ ë°˜ë³µ ë“±ì¥)
    brand_patterns = {
        'í…Œë¼': 'brands',
        'ì°¸ì´ìŠ¬': 'brands',
        'whisper': 'products',
    }
    
    # ì‚¬ëŒ (ë‹‰ë„¤ì„, í˜¸ì¹­)
    person_patterns = {
        'ì¹¨ì°©ë§¨': 'persons',
        'ë„‰ì‚´': 'persons',
        'ë„¥ì‚´': 'persons',
        'ì™€ì´í”„': 'persons',
    }
    
    # í† í”½
    topic_patterns = {
        'ê²Œì„': 'topics',
        'ë§¥ì£¼': 'topics',
        'ê´‘ê³ ': 'topics',
        'ë°©ì†¡': 'topics',
        'ë¼ì´ë¸Œ': 'topics',
    }
    
    all_patterns = {**brand_patterns, **person_patterns, **topic_patterns}
    
    for pattern, category in all_patterns.items():
        if pattern in text_lower:
            entities[category].append(pattern)
    
    return entities


def rank_keywords_by_relevance(word_counts: Counter, entities: dict, text: str) -> list:
    """
    ê´€ë ¨ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ë­í‚¹
    """
    scored_keywords = []
    
    # 1. ê³ ìœ ëª…ì‚¬ëŠ” ìš°ì„ ìˆœìœ„ ë¶€ì—¬
    priority_words = set()
    for category in entities.values():
        priority_words.update(category)
    
    # 2. ê° ë‹¨ì–´ì— ì ìˆ˜ ë¶€ì—¬
    for word, freq in word_counts.items():
        score = freq
        
        # ê³ ìœ ëª…ì‚¬ ê°€ì‚°ì 
        if word in priority_words:
            score *= 2.0
        
        # ê¸¸ì´ ê°€ì‚°ì  (2-6ê¸€ìê°€ ì ë‹¹)
        if 2 <= len(word) <= 6:
            score *= 1.2
        elif len(word) > 10:
            score *= 0.5
        
        # í•©ì„±ì–´ íŒë³„ (ë„ì–´ì“°ê¸° ì—†ì´ ë¶™ì€ ê²ƒ)
        if len(word) > 6 and not any(char in word for char in '0123456789'):
            score *= 0.7  # í˜ë„í‹°
        
        scored_keywords.append((word, freq, score))
    
    # ì ìˆ˜ìˆœ ì •ë ¬
    scored_keywords.sort(key=lambda x: x[2], reverse=True)
    
    return scored_keywords


def extract_keywords_contextual(text: str, top_n: int = 10, use_konlpy: bool = True) -> dict:
    """
    ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ë©”ì¸ í•¨ìˆ˜)
    """
    print("\nğŸ” ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...\n")
    
    # 1. ë©”íƒ€ë°ì´í„° ì œê±°
    print("  1ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì œê±°...")
    content = extract_metadata_and_content(text)
    
    # 2. ëª…ì‚¬ ì¶”ì¶œ
    print("  2ï¸âƒ£ ëª…ì‚¬ ì¶”ì¶œ ì¤‘...")
    if use_konlpy:
        nouns = extract_nouns_with_konlpy(content)
    else:
        nouns = None
    
    if not nouns:
        print("     â†’ ê°„ë‹¨í•œ íŒ¨í„´ ì‚¬ìš©")
        nouns = extract_nouns_simple(content)
    else:
        print(f"     âœ“ {len(nouns)}ê°œ ëª…ì‚¬ ì¶”ì¶œ")
    
    # 3. ë¶ˆìš©ì–´ ì œê±°
    print("  3ï¸âƒ£ ë¶ˆìš©ì–´ ì œê±°...")
    stopwords = get_extended_stopwords()
    filtered_nouns = [n for n in nouns if n not in stopwords]
    print(f"     âœ“ {len(filtered_nouns)}ê°œ ë‚¨ìŒ")
    
    # 4. ë¹ˆë„ í•„í„°ë§
    print("  4ï¸âƒ£ ë¹ˆë„ í•„í„°ë§...")
    word_counts = filter_by_frequency_and_length(filtered_nouns, min_freq=2)
    print(f"     âœ“ {len(word_counts)}ê°œ í›„ë³´")
    
    # 5. ê³ ìœ ëª…ì‚¬ íƒì§€
    print("  5ï¸âƒ£ ê³ ìœ ëª…ì‚¬ íƒì§€...")
    entities = detect_named_entities_simple(content)
    total_entities = sum(len(v) for v in entities.values())
    print(f"     âœ“ {total_entities}ê°œ ë°œê²¬")
    
    # 6. ê´€ë ¨ë„ ê¸°ë°˜ ë­í‚¹
    print("  6ï¸âƒ£ ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°...")
    ranked = rank_keywords_by_relevance(word_counts, entities, content)
    
    # ìµœì¢… ê²°ê³¼
    final_keywords = []
    for word, freq, score in ranked[:top_n]:
        kw_type = 'topic'
        for cat, words in entities.items():
            if word in words:
                kw_type = cat.rstrip('s')  # brands -> brand
                break
        
        final_keywords.append({
            'keyword': word,
            'frequency': freq,
            'score': round(score, 2),
            'type': kw_type
        })
    
    return {
        'keywords': final_keywords,
        'entities': entities,
        'total_nouns': len(nouns),
        'filtered_nouns': len(filtered_nouns),
        'candidates': len(word_counts)
    }


def main():
    parser = argparse.ArgumentParser(
        description='ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ STT ìµœì í™”)',
        epilog="""
ì˜ˆì‹œ:
  python extract_keywords_contextual.py input_denoised.txt
  python extract_keywords_contextual.py input.txt --top 15 --no-konlpy
  python extract_keywords_contextual.py input.txt --output result.json
        """
    )
    
    parser.add_argument('input_file', help='ì…ë ¥ íŒŒì¼ (denoised ê¶Œì¥)')
    parser.add_argument('--top', type=int, default=10, help='í‚¤ì›Œë“œ ê°œìˆ˜')
    parser.add_argument('--no-konlpy', action='store_true', help='í˜•íƒœì†Œ ë¶„ì„ ì•ˆí•¨')
    parser.add_argument('--output', help='JSON ì¶œë ¥')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ¯ ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ")
    print("=" * 80)
    
    # íŒŒì¼ ì½ê¸°
    print(f"\nğŸ“„ íŒŒì¼: {args.input_file}")
    with open(args.input_file, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"âœ“ í…ìŠ¤íŠ¸: {len(text):,} ê¸€ì")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    result = extract_keywords_contextual(
        text,
        top_n=args.top,
        use_konlpy=not args.no_konlpy
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print(f"âœ¨ ìµœì¢… í‚¤ì›Œë“œ Top {args.top}:")
    print("=" * 80)
    
    for i, kw in enumerate(result['keywords'], 1):
        print(f"{i:2d}. {kw['keyword']:15s} "
              f"[{kw['type']:8s}] "
              f"ë¹ˆë„:{kw['frequency']:2d}  "
              f"ì ìˆ˜:{kw['score']:5.1f}")
    
    # ê³ ìœ ëª…ì‚¬ ìš”ì•½
    if any(result['entities'].values()):
        print("\n" + "=" * 80)
        print("ğŸ·ï¸  ë°œê²¬ëœ ê³ ìœ ëª…ì‚¬:")
        print("=" * 80)
        for cat, words in result['entities'].items():
            if words:
                print(f"  {cat:12s}: {', '.join(words)}")
    
    # í†µê³„
    print("\n" + "=" * 80)
    print("ğŸ“Š ì¶”ì¶œ í†µê³„:")
    print("=" * 80)
    print(f"  ì „ì²´ ëª…ì‚¬: {result['total_nouns']}ê°œ")
    print(f"  ë¶ˆìš©ì–´ ì œê±° í›„: {result['filtered_nouns']}ê°œ")
    print(f"  ë¹ˆë„ í•„í„° í›„: {result['candidates']}ê°œ")
    print(f"  ìµœì¢… í‚¤ì›Œë“œ: {len(result['keywords'])}ê°œ")
    
    # JSON ì €ì¥
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ì €ì¥: {args.output}")
    
    print("\n" + "=" * 80)
    print("âœ… ì™„ë£Œ!")
    print("=" * 80)


if __name__ == '__main__':
    main()